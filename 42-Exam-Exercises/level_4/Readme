ft_split (my implementation) — how it works and why it’s structured this way
1) Delimiter model: isDelim

I treat “whitespace” as:

ASCII space (32)

ASCII control whitespace from \t to \r (9..13)

int isDelim(int c)
{
    return (c == 32 || (c >= 9 && c <= 13));
}


This makes the split behavior consistent with common whitespace tokenization and keeps the delimiter rule centralized (single source of truth).

2) First pass: counting words (word_count)

Before allocating anything, I compute how many tokens I will produce.

Core idea:

Skip delimiters

When I reach a non-delimiter, that’s the start of a word → increment count once

Then advance until the next delimiter (consume the whole word)

Repeat until '\0'

while (str[i])
{
    while (str[i] && isDelim(str[i])) i++;
    if (str[i] && !isDelim(str[i])) count++;
    while (str[i] && !isDelim(str[i])) i++;
}


Why this matters (memory discipline):

I can allocate the pointer table in one shot:

size = word_count(str);

malloc(sizeof(char *) * (size + 1))

+1 is required for the NULL terminator (result[size] = NULL), which is standard for char ** token arrays.

So the first allocation is exact and deterministic:

(number_of_words + 1) pointers.

3) Second pass: locate each word, allocate exact space, then fill

After I know size, I do a second traversal that builds each token.

a) Locate word boundaries using indices

I maintain:

j as the scanning index through str

start as the beginning index of the current word

Process per word:

Skip delimiters: while (str[j] && isDelim(str[j])) j++;

Mark start: start = j;

Advance to the end of the word: while (str[j] && !isDelim(str[j])) j++;

Compute length: w_size = j - start;

This is clean pointer/offset reasoning: length = end - start.

b) Memory allocation for one word: w_size + 1

Each word is allocated with:

result[i] = malloc((w_size + 1) * sizeof(char));


Why +1 is non-negotiable:
C strings must end with a null terminator.
So for a word of w_size characters, the memory layout is:

bytes [0 ... w_size-1] → characters

byte [w_size] → '\0'

c) Fill the allocated memory

Copy character-by-character:

while (w_size > k)
{
    result[i][k] = str[start + k];
    k++;
}
result[i][k] = '\0';


This demonstrates correct understanding of:

indexing as pointer arithmetic (start + k)

writing into heap memory

explicit string termination

What this implementation proves (and what you can say in the README)

I use a two-pass algorithm to allocate memory exactly:

count tokens

allocate + fill

I compute memory requirements before calling malloc:

pointer table: (word_count + 1) * sizeof(char *)

each word: (word_length + 1) * sizeof(char)

I explicitly manage how strings are built in memory:

allocate → fill bytes → null terminate

I enforce the standard C convention of a NULL-terminated array of strings

That’s the “expert/disciplined” story right there.

High-value improvements (optional, but they make it look even more professional)
1) Handle allocation failure without leaking memory

Right now, if result[i] fails, you return NULL but you don’t free previously allocated words or the result array.

A disciplined split usually includes a cleanup routine:

free result[0..i-1]

free result

This is the single biggest “expert” difference.

2) Remove unused variables

You have int start = 0; int w_size = 0; (good), but also int i, j, size etc. start/w_size are used, but j is doing the scanning and i counts words added (good). Just ensure everything present has a purpose. (Right now it mostly does.)

3) Minor style: sizeof(char) is always 1

This is not wrong:

malloc((w_size + 1) * sizeof(char));


But many C reviewers prefer:

malloc(w_size + 1);


Either is acceptable; keeping sizeof can communicate consistency.